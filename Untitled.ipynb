{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b75c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the required libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import lightgbm as lgb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Loading the dataset\n",
    "data = pd.read_excel(\"C:/Users/SHIVA KARTHIK P/OneDrive/Desktop/customer_churn_large_dataset.xlsx\")\n",
    "\n",
    "# Handling missing values by filling with mean\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "# Separating of features and target\n",
    "numerical_cols = ['Age', 'Subscription_Length_Months', 'Monthly_Bill', 'Total_Usage_GB']\n",
    "categorical_cols = ['Gender', 'Location']  # Assuming 'Name' and 'CustomerID' are not relevant\n",
    "\n",
    "X_numerical = data[numerical_cols]\n",
    "X_categorical = data[categorical_cols]\n",
    "\n",
    "y = data['Churn']\n",
    "\n",
    "# One-hot encoding of categorical variables\n",
    "encoder = OneHotEncoder()\n",
    "X_categorical_encoded = encoder.fit_transform(X_categorical)\n",
    "X_categorical_encoded = pd.DataFrame(X_categorical_encoded.toarray(),\n",
    "                                    columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "# Concatenating numerical and encoded categorical features\n",
    "X_final = pd.concat([X_numerical, X_categorical_encoded], axis=1)\n",
    "\n",
    "# Spliting of data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Applying feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Applying SMOTEENN for handling class imbalance\n",
    "sm = SMOTEENN()\n",
    "res_x, res_y = sm.fit_resample(X_final, y)\n",
    "rx_train, rx_test, ry_train, ry_test = train_test_split(res_x, res_y, test_size=0.2)\n",
    "\n",
    "# Training of LightGBM model\n",
    "train_data = lgb.Dataset(rx_train, label=ry_train)\n",
    "test_data = lgb.Dataset(rx_test, label=ry_test)\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 128,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'lambda_l1': 0.1,\n",
    "    'lambda_l2': 0.1\n",
    "}\n",
    "num_round = 1500\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])\n",
    "\n",
    "# Making predictions using LightGBM\n",
    "y_pred_lightgbm = bst.predict(rx_test)\n",
    "\n",
    "# Training Neural Network\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=rx_train.shape[1], activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(rx_train, ry_train, epochs=150, batch_size=64, validation_data=(rx_test, ry_test), verbose=1)\n",
    "\n",
    "# Making predictions using Neural Network\n",
    "y_pred_nn = model.predict(rx_test)\n",
    "\n",
    "# Combining predictions\n",
    "combined_predictions = (y_pred_lightgbm + y_pred_nn.flatten()) / 2\n",
    "y_pred_combined_binary = [1 if p >= 0.5 else 0 for p in combined_predictions]\n",
    "\n",
    "# Evaluation of the combined model using accuracy\n",
    "combined_accuracy = accuracy_score(ry_test, y_pred_combined_binary)\n",
    "print(\"Combined Model Accuracy:\", combined_accuracy)\n",
    "\n",
    "# Geting feature importance from LightGBM\n",
    "importance = bst.feature_importance()\n",
    "feature_names = X_final.columns.tolist()\n",
    "feature_importance = pd.DataFrame({'feature': feature_names, 'importance': importance})\n",
    "feature_importance = feature_importance.sort_values(by='importance', ascending=False)\n",
    "print(\"Feature Importance from LightGBM:\", feature_importance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4590b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Pickle \n",
    "import pickle\n",
    "\n",
    "filename='model.churn'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f034cb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model\n",
    "pickle.dump(model,open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e099741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verifing saved model\n",
    "load_model=pickle.load(open(filename,'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
